{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e8544-0d44-4da1-b990-8f31f31f4b86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import json\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "\n",
    "# Get Redshift cluster information\n",
    "redshift = boto3.client('redshift')\n",
    "clusters = redshift.describe_clusters()\n",
    "\n",
    "# Print cluster endpoint and security group information\n",
    "for cluster in clusters['Clusters']:\n",
    "    print(f\"Cluster Identifier: {cluster['ClusterIdentifier']}\")\n",
    "    print(f\"Endpoint: {cluster['Endpoint']['Address']}:{cluster['Endpoint']['Port']}\")\n",
    "    print(f\"Security Groups: {cluster['VpcSecurityGroups']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c165c62-562b-45a2-82d0-5d2351813a21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_secret(secret_name, region_name=\"us-east-2\"):\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "        return json.loads(get_secret_value_response['SecretString'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving secret: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e2566-cdc1-4c33-bb38-2b41803f54f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute_redshift_query(cluster_id, database, db_user, query):\n",
    "    redshift_client = boto3.client('redshift-data')\n",
    "    \n",
    "    try:\n",
    "        response = redshift_client.execute_statement(\n",
    "            ClusterIdentifier='Cluster_ID',\n",
    "            Database=database,\n",
    "            DbUser=db_user,\n",
    "            Sql=query\n",
    "        )\n",
    "        \n",
    "        query_id = response['Id']\n",
    "        \n",
    "        while True:\n",
    "            status_response = redshift_client.describe_statement(Id=query_id)\n",
    "            status = status_response['Status']\n",
    "            \n",
    "            if status == 'FINISHED':\n",
    "                print(f\"Query completed successfully: {query}\")\n",
    "                # Get the results if it's a SELECT query\n",
    "                if query.strip().upper().startswith('SELECT'):\n",
    "                    result = redshift_client.get_statement_result(Id=query_id)\n",
    "                    return result\n",
    "                return True\n",
    "            elif status == 'FAILED':\n",
    "                print(f\"Query failed: {status_response.get('Error', 'Unknown error')}\")\n",
    "                return False\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505675c-6903-4dc9-a9ba-1a5c3746ac00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get credentials from Secrets Manager\n",
    "    secret_name = \"Redshift-redshift-secret\"\n",
    "    credentials = get_secret(secret_name)\n",
    "    \n",
    "    username = credentials.get('username')\n",
    "    \n",
    "    # Skip database creation since it already exists\n",
    "    print(\"Database 'analytics' already exists, proceeding with schema creation...\")\n",
    "    \n",
    "    # Create schema\n",
    "    print(\"Creating schema...\")\n",
    "    create_schema_query = \"CREATE SCHEMA IF NOT EXISTS analytics_schema;\"\n",
    "    success = execute_redshift_query('Cluster_ID', 'analytics', username, create_schema_query)\n",
    "    \n",
    "    if success:\n",
    "        # Create tables\n",
    "        create_tables_queries = [\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS analytics_schema.customers (\n",
    "                customer_id INTEGER PRIMARY KEY,\n",
    "                name VARCHAR(100),\n",
    "                email VARCHAR(100),\n",
    "                created_date TIMESTAMP\n",
    "            );\n",
    "            \"\"\",\n",
    "            \n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS analytics_schema.orders (\n",
    "                order_id INTEGER PRIMARY KEY,\n",
    "                customer_id INTEGER,\n",
    "                order_date TIMESTAMP,\n",
    "                total_amount DECIMAL(10,2),\n",
    "                FOREIGN KEY (customer_id) REFERENCES analytics_schema.customers(customer_id)\n",
    "            );\n",
    "            \"\"\"\n",
    "        ]\n",
    "        \n",
    "        print(\"Creating tables...\")\n",
    "        for query in create_tables_queries:\n",
    "            success = execute_redshift_query('Cluster_ID', 'analytics', username, query)\n",
    "            if not success:\n",
    "                print(\"Failed to create tables\")\n",
    "                break\n",
    "        \n",
    "        # Insert sample data\n",
    "        insert_queries = [\n",
    "            \"\"\"\n",
    "            INSERT INTO analytics_schema.customers (customer_id, name, email, created_date)\n",
    "            VALUES \n",
    "                (1, 'John Doe', 'john@example.com', CURRENT_TIMESTAMP),\n",
    "                (2, 'Jane Smith', 'jane@example.com', CURRENT_TIMESTAMP);\n",
    "            \"\"\",\n",
    "            \n",
    "            \"\"\"\n",
    "            INSERT INTO analytics_schema.orders (order_id, customer_id, order_date, total_amount)\n",
    "            VALUES \n",
    "                (1, 1, CURRENT_TIMESTAMP, 100.50),\n",
    "                (2, 1, CURRENT_TIMESTAMP, 200.75),\n",
    "                (3, 2, CURRENT_TIMESTAMP, 150.25);\n",
    "            \"\"\"\n",
    "        ]\n",
    "        \n",
    "        print(\"Inserting sample data...\")\n",
    "        for query in insert_queries:\n",
    "            success = execute_redshift_query('Cluster_ID', 'analytics', username, query)\n",
    "            if not success:\n",
    "                print(\"Failed to insert sample data\")\n",
    "                break\n",
    "        \n",
    "        # Verify the data\n",
    "        verify_queries = [\n",
    "            \"SELECT COUNT(*) FROM analytics_schema.customers;\",\n",
    "            \"SELECT COUNT(*) FROM analytics_schema.orders;\"\n",
    "        ]\n",
    "        \n",
    "        print(\"Verifying data...\")\n",
    "        for query in verify_queries:\n",
    "            result = execute_redshift_query('Cluster_ID', 'analytics', username, query)\n",
    "            if isinstance(result, dict):\n",
    "                print(f\"Query {query} result:\", result)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e744804-c5b7-4b91-8956-15b0bff021aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_redshift_data_api():\n",
    "    redshift_data = boto3.client('redshift-data')\n",
    "    \n",
    "    try:\n",
    "        # Execute a simple query\n",
    "        response = redshift_data.execute_statement(\n",
    "            ClusterIdentifier='Cluster_ID',\n",
    "            Database='analytics',\n",
    "            DbUser=credentials['username'],\n",
    "            Sql='SELECT COUNT(*) FROM analytics_schema.customers'\n",
    "        )\n",
    "        \n",
    "        query_id = response['Id']\n",
    "        \n",
    "        # Wait for query completion\n",
    "        while True:\n",
    "            status = redshift_data.describe_statement(Id=query_id)\n",
    "            if status['Status'] == 'FINISHED':\n",
    "                result = redshift_data.get_statement_result(Id=query_id)\n",
    "                print(\"Data API connection successful!\")\n",
    "                print(f\"Query result: {result}\")\n",
    "                break\n",
    "            elif status['Status'] == 'FAILED':\n",
    "                print(f\"Query failed: {status.get('Error')}\")\n",
    "                break\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Data API error: {str(e)}\")\n",
    "\n",
    "test_redshift_data_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab12f3f5-a573-4133-bd69-e493d0a950d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_value(value, data_type):\n",
    "    if value is None:\n",
    "        return None\n",
    "    \n",
    "    if isinstance(data_type, TimestampType):\n",
    "        # Convert string timestamp to datetime\n",
    "        try:\n",
    "            return datetime.strptime(value, '%Y-%m-%d %H:%M:%S.%f')\n",
    "        except ValueError:\n",
    "            try:\n",
    "                return datetime.strptime(value, '%Y-%m-%d %H:%M:%S')\n",
    "            except ValueError:\n",
    "                return None\n",
    "    elif isinstance(data_type, DecimalType):\n",
    "        return float(value)\n",
    "    elif isinstance(data_type, LongType):\n",
    "        return int(value)\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df832c7-c6e1-435e-bac7-3e9bf32e3573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_redshift_with_data_api(query):\n",
    "    redshift_data = boto3.client('redshift-data')\n",
    "    \n",
    "    try:\n",
    "        # Execute query\n",
    "        response = redshift_data.execute_statement(\n",
    "            ClusterIdentifier='Cluster_ID',\n",
    "            Database='analytics',\n",
    "            DbUser=credentials['username'],\n",
    "            Sql=query\n",
    "        )\n",
    "        \n",
    "        query_id = response['Id']\n",
    "        \n",
    "        # Wait for completion\n",
    "        while True:\n",
    "            status = redshift_data.describe_statement(Id=query_id)\n",
    "            if status['Status'] == 'FINISHED':\n",
    "                result = redshift_data.get_statement_result(Id=query_id)\n",
    "                \n",
    "                # Get column metadata\n",
    "                column_metadata = result['ColumnMetadata']\n",
    "                \n",
    "                # Create schema based on column metadata\n",
    "                schema = StructType([\n",
    "                    StructField(\n",
    "                        col['name'],\n",
    "                        {\n",
    "                            'int8': LongType(),\n",
    "                            'varchar': StringType(),\n",
    "                            'timestamp': TimestampType(),\n",
    "                            'decimal': DecimalType(10, 2),\n",
    "                        }.get(col['typeName'].lower(), StringType()),\n",
    "                        True\n",
    "                    ) for col in column_metadata\n",
    "                ])\n",
    "                \n",
    "                # Convert result to list of rows with proper type conversion\n",
    "                data = []\n",
    "                for record in result['Records']:\n",
    "                    row = []\n",
    "                    for field, col_meta, schema_field in zip(record, column_metadata, schema.fields):\n",
    "                        # Get the first non-None value from the field\n",
    "                        value = next((v for k, v in field.items() if v is not None), None)\n",
    "                        # Convert the value to the appropriate type\n",
    "                        converted_value = convert_value(value, schema_field.dataType)\n",
    "                        row.append(converted_value)\n",
    "                    data.append(row)\n",
    "                \n",
    "                # Create Spark DataFrame\n",
    "                spark = SparkSession.builder \\\n",
    "                    .appName(\"Redshift Data API\") \\\n",
    "                    .getOrCreate()\n",
    "                \n",
    "                return spark.createDataFrame(data, schema)\n",
    "                \n",
    "            elif status['Status'] == 'FAILED':\n",
    "                raise Exception(f\"Query failed: {status.get('Error')}\")\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdfab4-6ad7-45b5-a2ea-9cb474113adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test reading data\n",
    "try:\n",
    "    # Read customers\n",
    "    print(\"Reading customers table...\")\n",
    "    customers_df = read_redshift_with_data_api(\"\"\"\n",
    "        SELECT * FROM analytics_schema.customers\n",
    "    \"\"\")\n",
    "    print(\"\\nCustomers data:\")\n",
    "    customers_df.show()\n",
    "    \n",
    "    # Print schema to verify data types\n",
    "    print(\"\\nCustomers schema:\")\n",
    "    customers_df.printSchema()\n",
    "    \n",
    "    # Read orders\n",
    "    print(\"\\nReading orders table...\")\n",
    "    orders_df = read_redshift_with_data_api(\"\"\"\n",
    "        SELECT * FROM analytics_schema.orders\n",
    "    \"\"\")\n",
    "    print(\"\\nOrders data:\")\n",
    "    orders_df.show()\n",
    "    \n",
    "    # Print schema to verify data types\n",
    "    print(\"\\nOrders schema:\")\n",
    "    orders_df.printSchema()\n",
    "    \n",
    "    # Example of joining data\n",
    "    print(\"\\nJoining customers and orders...\")\n",
    "    joined_df = customers_df.join(\n",
    "        orders_df,\n",
    "        customers_df.customer_id == orders_df.customer_id,\n",
    "        \"left\"\n",
    "    )\n",
    "    print(\"\\nJoined data:\")\n",
    "    joined_df.show()\n",
    "    \n",
    "    # Example of aggregation\n",
    "    print(\"\\nCalculating total orders per customer...\")\n",
    "    agg_df = orders_df.groupBy(\"customer_id\") \\\n",
    "        .agg({\"total_amount\": \"sum\"}) \\\n",
    "        .withColumnRenamed(\"sum(total_amount)\", \"total_spent\")\n",
    "    print(\"\\nAggregated data:\")\n",
    "    agg_df.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336fa7a-3bdb-434c-a96e-5ee0167a5e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
